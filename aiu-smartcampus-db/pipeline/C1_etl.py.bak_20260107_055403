#!/usr/bin/env python3
"""
C1_etl.py — Real-time ETL pipeline (file-based stream simulation)

Sources (pipeline/inbox):
- student_profiles.jsonl  -> MongoDB (aiu_smartcampus.student_profiles) + MySQL dimension (departments, students)
- activity_logs.csv       -> MySQL (activity_logs)
- sensor_readings.csv     -> ClickHouse (aiu_timeseries.sensor_readings_raw)
- club_memberships.csv    -> Neo4j (MERGE Student/Club + MEMBER_OF)

Features:
- incremental checkpoints (byte offsets) in pipeline/state/checkpoint.json
- validation + bad row capture
- logging + proof-friendly stdout summary
"""

from __future__ import annotations

import csv
import json
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import pymysql
from pymongo import MongoClient, UpdateOne
import clickhouse_connect
from neo4j import GraphDatabase


ROOT = Path(__file__).resolve().parents[1]
INBOX = ROOT / "pipeline" / "inbox"
STATE = ROOT / "pipeline" / "state"
BAD   = ROOT / "pipeline" / "bad_rows"
BAD.mkdir(parents=True, exist_ok=True)

CHECKPOINT_PATH = STATE / "checkpoint.json"


def env(name: str, default: str = "") -> str:
    v = os.environ.get(name)
    return v if v not in (None, "") else default


def load_checkpoint() -> dict[str, int]:
    if CHECKPOINT_PATH.exists():
        return json.loads(CHECKPOINT_PATH.read_text(encoding="utf-8"))
    return {}


def save_checkpoint(cp: dict[str, int]) -> None:
    STATE.mkdir(parents=True, exist_ok=True)
    CHECKPOINT_PATH.write_text(json.dumps(cp, indent=2), encoding="utf-8")


def read_new_bytes(path: Path, last: int) -> tuple[str, int]:
    if not path.exists():
        return "", last
    data = path.read_bytes()
    if last >= len(data):
        return "", last
    chunk = data[last:]
    return chunk.decode("utf-8", errors="replace"), len(data)


def write_bad(source: str, rows: list[dict[str, Any]]) -> None:
    if not rows:
        return
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    out = BAD / f"{source}_bad_{ts}.jsonl"
    with out.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


# -------------------------
# Connections
# -------------------------
def mysql_conn():
    host = env("MYSQL_HOST", "db")
    port = int(env("MYSQL_PORT", "3306"))
    pwd  = env("MYSQL_ROOT_PASSWORD", "")
    # prefer MYSQL_DATABASE if correct; fallback to aiu_urms_ext
    db1  = env("MYSQL_DATABASE", "aiu_urms_ext")
    user = env("MYSQL_USER", "root")

    conn = pymysql.connect(
        host=host, port=port, user=user, password=pwd,
        autocommit=False, charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor
    )
    with conn.cursor() as cur:
        cur.execute(f"CREATE DATABASE IF NOT EXISTS `{db1}`;")
        cur.execute(f"USE `{db1}`;")
    conn.commit()
    return conn, db1


def ensure_mysql_tables(conn) -> None:
    # minimal tables needed for ETL (match your schema shape)
    ddl = [
        """
        CREATE TABLE IF NOT EXISTS departments (
          department_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
          name VARCHAR(100) NOT NULL,
          CONSTRAINT uq_departments_name UNIQUE (name)
        ) ENGINE=InnoDB;
        """,
        """
        CREATE TABLE IF NOT EXISTS students (
          student_id INT UNSIGNED PRIMARY KEY,
          reg_no VARCHAR(50) NOT NULL,
          full_name VARCHAR(150) NOT NULL,
          email VARCHAR(150) NOT NULL,
          phone VARCHAR(30),
          department_id INT UNSIGNED NOT NULL,
          year_of_study TINYINT UNSIGNED NOT NULL,
          CONSTRAINT uq_students_reg UNIQUE (reg_no),
          CONSTRAINT uq_students_email UNIQUE (email),
          CONSTRAINT fk_students_dept FOREIGN KEY (department_id) REFERENCES departments(department_id)
        ) ENGINE=InnoDB;
        """,
        """
        CREATE TABLE IF NOT EXISTS activity_logs (
          log_id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
          student_id INT UNSIGNED NOT NULL,
          activity_type ENUM('login','logout','view_material','submit_assignment') NOT NULL,
          resource_id VARCHAR(100),
          timestamp DATETIME NOT NULL,
          session_duration INT UNSIGNED DEFAULT 0,
          ip_address VARCHAR(45),
          INDEX idx_activity_student_time (student_id, timestamp),
          CONSTRAINT fk_logs_student FOREIGN KEY (student_id) REFERENCES students(student_id)
        ) ENGINE=InnoDB;
        """
    ]
    with conn.cursor() as cur:
        for q in ddl:
            cur.execute(q)
    conn.commit()


def mongo_conn():
    host = env("MONGO_HOST", "mongo")
    port = int(env("MONGO_PORT", "27017"))
    user = env("MONGO_INITDB_ROOT_USERNAME", "root")
    pwd  = env("MONGO_INITDB_ROOT_PASSWORD", "rootpass")
    client = MongoClient(f"mongodb://{user}:{pwd}@{host}:{port}/admin")
    db = client["aiu_smartcampus"]
    return client, db


def clickhouse_client():
    host = env("CLICKHOUSE_HOST", "clickhouse")
    port = int(env("CLICKHOUSE_PORT", "8123"))
    user = env("CLICKHOUSE_USER", "ch_admin")
    pwd  = env("CLICKHOUSE_PASSWORD", "chpass123")
    db   = env("CLICKHOUSE_DB", "aiu_timeseries")
    return clickhouse_connect.get_client(host=host, port=port, username=user, password=pwd, database=db, secure=False)


def neo4j_driver():
    host = env("NEO4J_HOST", "neo4j")
    port = int(env("NEO4J_BOLT_INNER_PORT", "7687"))  # inside docker network
    auth = env("NEO4J_AUTH", "neo4j/neo4jpass123")
    neo_user, neo_pass = auth.split("/", 1)
    uri = f"bolt://{host}:{port}"
    return GraphDatabase.driver(uri, auth=(neo_user, neo_pass))


# -------------------------
# ETL: student profiles
# -------------------------
def etl_student_profiles(cp: dict[str, int], mysql, mongo_db) -> tuple[int, int]:
    src = INBOX / "student_profiles.jsonl"
    last = cp.get("student_profiles.jsonl", 0)
    text, newpos = read_new_bytes(src, last)
    if not text.strip():
        cp["student_profiles.jsonl"] = newpos
        return 0, 0

    ok, bad = [], []
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            doc = json.loads(line)
            # required
            for k in ("student_id", "reg_no", "full_name", "department", "year_of_study", "email"):
                if k not in doc or doc[k] in (None, ""):
                    raise ValueError(f"missing {k}")
            doc["student_id"] = int(doc["student_id"])
            doc["year_of_study"] = int(doc["year_of_study"])
            ok.append(doc)
        except Exception as e:
            bad.append({"raw": line, "error": str(e)})

    if bad:
        write_bad("student_profiles", bad)

    # Mongo upsert
    if ok:
        ops = [UpdateOne({"student_id": d["student_id"]}, {"$set": d}, upsert=True) for d in ok]
        mongo_db["student_profiles"].bulk_write(ops, ordered=False)

    # MySQL upsert (departments + students)
    inserted = 0
    with mysql.cursor() as cur:
        # departments
        dept_names = sorted({d["department"] for d in ok})
        for name in dept_names:
            cur.execute("INSERT IGNORE INTO departments(name) VALUES (%s);", (name,))
        # map dept -> id
        cur.execute("SELECT department_id, name FROM departments;")
        dept_map = {r["name"]: r["department_id"] for r in cur.fetchall()}

        for d in ok:
            dept_id = dept_map[d["department"]]
            cur.execute(
                """
                INSERT INTO students(student_id, reg_no, full_name, email, phone, department_id, year_of_study)
                VALUES (%s,%s,%s,%s,%s,%s,%s)
                ON DUPLICATE KEY UPDATE
                  reg_no=VALUES(reg_no),
                  full_name=VALUES(full_name),
                  email=VALUES(email),
                  phone=VALUES(phone),
                  department_id=VALUES(department_id),
                  year_of_study=VALUES(year_of_study);
                """,
                (d["student_id"], d["reg_no"], d["full_name"], d["email"], d.get("phone"), dept_id, d["year_of_study"])
            )
            inserted += 1

    mysql.commit()
    cp["student_profiles.jsonl"] = newpos
    return inserted, len(bad)


# -------------------------
# ETL: activity logs
# -------------------------
def etl_activity_logs(cp: dict[str, int], mysql) -> tuple[int, int]:
    src = INBOX / "activity_logs.csv"
    last = cp.get("activity_logs.csv", 0)
    text, newpos = read_new_bytes(src, last)
    if not text.strip():
        cp["activity_logs.csv"] = newpos
        return 0, 0

    ok_rows, bad = [], []
    rdr = csv.DictReader(text.splitlines())
    for r in rdr:
        try:
            sid = int(r["student_id"])
            at  = (r["activity_type"] or "").strip()
            if at not in ("login","logout","view_material","submit_assignment"):
                raise ValueError("bad activity_type")
            ts = datetime.strptime(r["timestamp"].strip(), "%Y-%m-%d %H:%M:%S")
            dur = int(r.get("session_duration") or 0)
            ok_rows.append((sid, at, (r.get("resource_id") or None), ts.strftime("%Y-%m-%d %H:%M:%S"), dur, (r.get("ip_address") or None)))
        except Exception as e:
            bad.append({"raw": r, "error": str(e)})

    if bad:
        write_bad("activity_logs", bad)

    inserted = 0
    try:
        with mysql.cursor() as cur:
            cur.executemany(
                """
                INSERT INTO activity_logs(student_id, activity_type, resource_id, timestamp, session_duration, ip_address)
                VALUES (%s,%s,%s,%s,%s,%s);
                """,
                ok_rows
            )
            inserted = cur.rowcount
        mysql.commit()
    except Exception:
        mysql.rollback()
        raise

    cp["activity_logs.csv"] = newpos
    return inserted, len(bad)


# -------------------------
# ETL: ClickHouse sensor readings
# -------------------------
def etl_sensor_readings(cp: dict[str, int], ch) -> tuple[int, int]:
    src = INBOX / "sensor_readings.csv"
    last = cp.get("sensor_readings.csv", 0)
    text, newpos = read_new_bytes(src, last)
    if not text.strip():
        cp["sensor_readings.csv"] = newpos
        return 0, 0

    ok, bad = [], []
    rdr = csv.DictReader(text.splitlines())
    for r in rdr:
        try:
            ok.append((
                int(r["sensor_id"]),
                int(r["room_id"]),
                r["sensor_type"].strip(),
                r["ts"].strip(),   # clickhouse-connect accepts str datetime
                float(r["value"]),
                r.get("status","ok").strip()
            ))
        except Exception as e:
            bad.append({"raw": r, "error": str(e)})

    if bad:
        write_bad("sensor_readings", bad)

    if ok:
        ch.insert(
            "aiu_timeseries.sensor_readings_raw",
            ok,
            column_names=["sensor_id","room_id","sensor_type","ts","value","status"]
        )

    cp["sensor_readings.csv"] = newpos
    return len(ok), len(bad)


# -------------------------
# ETL: Neo4j club memberships
# -------------------------
def etl_club_memberships(cp: dict[str, int], driver) -> tuple[int, int]:
    src = INBOX / "club_memberships.csv"
    last = cp.get("club_memberships.csv", 0)
    text, newpos = read_new_bytes(src, last)
    if not text.strip():
        cp["club_memberships.csv"] = newpos
        return 0, 0

    ok, bad = [], []
    rdr = csv.DictReader(text.splitlines())
    for r in rdr:
        try:
            reg = (r["reg_no"] or "").strip()
            club = (r["club"] or "").strip()
            if not reg or not club:
                raise ValueError("missing reg_no/club")
            ok.append({"reg_no": reg, "club": club})
        except Exception as e:
            bad.append({"raw": r, "error": str(e)})

    if bad:
        write_bad("club_memberships", bad)

    if ok:
        q = """
        UNWIND $rows AS row
        MERGE (s:Student {reg_no: row.reg_no})
        MERGE (c:Club {name: row.club})
        MERGE (s)-[:MEMBER_OF]->(c);
        """
        with driver.session() as sess:
            sess.execute_write(lambda tx: tx.run(q, rows=ok).consume())

    cp["club_memberships.csv"] = newpos
    return len(ok), len(bad)


def main() -> int:
    print("=== C1 ETL START ===")
    cp = load_checkpoint()

    mysql, mysql_db = mysql_conn()
    ensure_mysql_tables(mysql)

    mongo_client, mongo_db = mongo_conn()
    ch = clickhouse_client()
    neo = neo4j_driver()

    try:
        sp_ok, sp_bad = etl_student_profiles(cp, mysql, mongo_db)
        al_ok, al_bad = etl_activity_logs(cp, mysql)
        sr_ok, sr_bad = etl_sensor_readings(cp, ch)
        cm_ok, cm_bad = etl_club_memberships(cp, neo)

        save_checkpoint(cp)

        # proof counts
        with mysql.cursor() as cur:
            cur.execute("SELECT COUNT(*) AS n FROM departments;")
            dept_n = cur.fetchone()["n"]
            cur.execute("SELECT COUNT(*) AS n FROM students;")
            stu_n = cur.fetchone()["n"]
            cur.execute("SELECT COUNT(*) AS n FROM activity_logs;")
            log_n = cur.fetchone()["n"]

        mongo_n = mongo_db["student_profiles"].count_documents({})

        ch_n = ch.query("SELECT count() FROM aiu_timeseries.sensor_readings_raw").result_rows[0][0]

        with neo.session() as sess:
            r = sess.run("MATCH (s:Student) RETURN count(s) AS n").single()
            neo_students = r["n"]
            r = sess.run("MATCH (c:Club) RETURN count(c) AS n").single()
            neo_clubs = r["n"]

        print("--- ETL RESULT ---")
        print(f"MySQL db={mysql_db} departments={dept_n} students={stu_n} activity_logs={log_n}")
        print(f"Mongo student_profiles={mongo_n}")
        print(f"ClickHouse sensor_readings_raw={ch_n}")
        print(f"Neo4j Students={neo_students} Clubs={neo_clubs}")

        print("--- BATCH STATS ---")
        print(f"student_profiles ok={sp_ok} bad={sp_bad}")
        print(f"activity_logs    ok={al_ok} bad={al_bad}")
        print(f"sensor_readings  ok={sr_ok} bad={sr_bad}")
        print(f"club_memberships ok={cm_ok} bad={cm_bad}")

        print("=== C1 ETL END ===")
        return 0

    finally:
        try: mysql.close()
        except: pass
        try: mongo_client.close()
        except: pass
        try: neo.close()
        except: pass


if __name__ == "__main__":
    raise SystemExit(main())
